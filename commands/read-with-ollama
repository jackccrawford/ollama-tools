#!/bin/bash
# read-with-ollama - A simple wrapper script for ollama_file_reader.py

# Set default model
MODEL="llama3.1"

# Help text
function show_help {
    echo "Usage: read-with-ollama [OPTIONS]"
    echo ""
    echo "Chat with an Ollama model that can read files using function calling."
    echo ""
    echo "Options:"
    echo "  -m, --model MODEL    Specify which Ollama model to use (default: llama3.1)"
    echo "  -l, --list           List available models"
    echo "  -h, --help           Show this help message"
    echo ""
    echo "Examples:"
    echo "  read-with-ollama"
    echo "  read-with-ollama --model llama3.1"
    echo "  read-with-ollama --model mistral-nemo"
    echo ""
    echo "Once running, you can ask questions like:"
    echo "  - What's in the file /path/to/file.txt?"
    echo "  - List the files in /some/directory"
    echo "  - Tell me about the file /path/to/document.md"
    echo ""
}

# List available models
function list_models {
    echo "Available Ollama models:"
    ollama list
    echo ""
    echo "Note: Only some models support function calling. Known compatible models include:"
    echo "  - llama3.1"
    echo "  - mistral-nemo"
    echo "  - firefunction-v2 (if installed)"
    echo "  - command-r-plus (if installed)"
    echo ""
}

# Parse command line options
while [[ $# -gt 0 ]]; do
    case "$1" in
        -m|--model)
            MODEL="$2"
            shift 2
            ;;
        -l|--list)
            list_models
            exit 0
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            show_help
            exit 1
            ;;
    esac
done

# Check if Python 3 is installed
if ! command -v python3 &> /dev/null; then
    echo "Error: Python 3 is required but not installed."
    exit 1
fi

# Check if the ollama package is installed
if ! python3 -c "import ollama" &> /dev/null; then
    echo "Error: The 'ollama' Python package is not installed."
    echo "Installing it now..."
    pip install ollama
fi

# Check if the Ollama service is running
if ! curl -s http://localhost:11434/api/version &> /dev/null; then
    echo "Error: The Ollama service is not running."
    echo "Please start Ollama with 'ollama serve' and try again."
    exit 1
fi

# Make sure the model exists
if ! ollama list | grep -q "$MODEL"; then
    echo "Model '$MODEL' not found. Attempting to pull it..."
    ollama pull "$MODEL"
fi

# Run the Python script
python3 -m ollama_file_reader --model "$MODEL"
